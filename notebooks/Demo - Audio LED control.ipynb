{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54dc6b14-60fa-48a6-9bc4-aca05fd805f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay\n",
    "ol = Overlay(\"base.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "471bb0ec-2fc1-4ff5-9241-b44f8402914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import traitlets\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import cv2\n",
    "import math\n",
    "from colour import Color\n",
    "from matplotlib import pyplot as plt\n",
    "import asyncio\n",
    "\n",
    "from hearlight.audio import audio_led_driver_setup\n",
    "\n",
    "from pynq import allocate\n",
    "\n",
    "\"\"\"Test class for software audio processing with CNN\n",
    "\n",
    "\"\"\"\n",
    "class AudioProcessSoftwareCNN(traitlets.HasTraits):\n",
    "    weights_trait = traitlets.Dict()\n",
    "    cnn_version_trait = traitlets.Int()\n",
    "    \n",
    "    # traits which will be linked to main control panel if dashboard is used (after initialisation of the class)\n",
    "    led_max_current_trait = traitlets.Float()\n",
    "    channel_max_current_trait = traitlets.Float()\n",
    "    switch_max_current_trait = traitlets.Float()\n",
    "    device_max_current_trait = traitlets.Float()\n",
    "    dac_ref_trait = traitlets.Float()\n",
    "    irr_to_current_trait = traitlets.Dict()\n",
    "    current_to_irr_trait = traitlets.Dict()\n",
    "    \n",
    "    def __init__(self, panel):\n",
    "        self.panel = panel\n",
    "        \n",
    "        self.panel.start_button.on_click(self._start_button_clicked)\n",
    "        \n",
    "        # file upload for weights selection\n",
    "        self.weights_file_upload = widgets.FileUpload(accept='.dat', multiple=True)\n",
    "        traitlets.link((self.weights_file_upload, 'value'), (self, 'weights_trait'))\n",
    "        \n",
    "        # dropdown for selecting nn version\n",
    "        self.cnn_version_dropdown = widgets.Dropdown(options=[('v1', 1), ('v2', 2), ('v3', 3)], layout={'width' :  '150px'})\n",
    "        traitlets.link((self.cnn_version_dropdown, 'value'), (self, 'cnn_version_trait'))\n",
    "\n",
    "        self.panel.processor_settings = {'Select weights file: ' : self.weights_file_upload,\n",
    "                                         'Select CNN version: ' : self.cnn_version_dropdown}\n",
    "        \n",
    "        self.panel.fig_ft = widgets.HTML(value='<i style=\"color:white;\">Put a spectrogram here...</i>')\n",
    "        \n",
    "    def _start_button_clicked(self, button):\n",
    "        self.run_task = asyncio.ensure_future(self.run())\n",
    "        \n",
    "    async def run(self):\n",
    "        \"\"\"Do the CNN demo...\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "\"\"\"Class for software audio processing with FFT\n",
    "\n",
    "\"\"\"\n",
    "class AudioProcessSoftwareFFT(traitlets.HasTraits):\n",
    "    fs_trait = traitlets.Int()\n",
    "    time_to_run_trait = traitlets.Int()\n",
    "    n_leds_trait = traitlets.Int()\n",
    "    n_fft_bins_trait = traitlets.Int()\n",
    "    min_frequency_trait = traitlets.Float()\n",
    "    max_frequency_trait = traitlets.Float()\n",
    "    n_samples_window_trait = traitlets.Int()\n",
    "    \n",
    "    # traits which will be linked to main control panel if dashboard is used (after initialisation of the class)\n",
    "    led_max_current_trait = traitlets.Float()\n",
    "    channel_max_current_trait = traitlets.Float()\n",
    "    switch_max_current_trait = traitlets.Float()\n",
    "    device_max_current_trait = traitlets.Float()\n",
    "    dac_ref_trait = traitlets.Float()\n",
    "    irr_to_current_trait = traitlets.Dict()\n",
    "    current_to_irr_trait = traitlets.Dict()\n",
    "    \n",
    "    def __init__(self, panel):\n",
    "        self.panel = panel\n",
    "        \n",
    "        # download the base overlay\n",
    "        self.ol = ol\n",
    "        \n",
    "        self.panel.observe(self._setup_fft_plot, ['fs_trait'])\n",
    "        self.panel.observe(self.setup_demo, ['fs_trait'])\n",
    "        \n",
    "        self.panel.start_button.on_click(self._start_button_clicked)\n",
    "        \n",
    "        # toggle button for stop button override\n",
    "        self.panel.stop_button = widgets.ToggleButton(description='STOP', icon='stop')\n",
    "        self.panel.stop_button.add_class('stop_button')\n",
    "\n",
    "        # text entry for time to run\n",
    "        self.time_to_run_entry = widgets.IntText(value=10, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.time_to_run_entry, 'value'), (self, 'time_to_run_trait'))\n",
    "        \n",
    "        # text entry for number of LEDs\n",
    "        self.n_leds_entry = widgets.IntText(value=1, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.n_leds_entry, 'value'), (self, 'n_leds_trait'))\n",
    "        \n",
    "        # text entry for number of FFT bins\n",
    "        self.n_fft_bins_entry = widgets.IntText(value=200, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.n_fft_bins_entry, 'value'), (self, 'n_fft_bins_trait'))\n",
    "        self.observe(self._setup_fft_plot, ['n_fft_bins_trait'])\n",
    "        self.observe(self.setup_demo, ['n_fft_bins_trait'])\n",
    "                \n",
    "        # text entry for min frequency\n",
    "        self.min_frequency_entry = widgets.FloatText(value=4000, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.min_frequency_entry, 'value'), (self, 'min_frequency_trait'))\n",
    "        self.observe(self.tonotopic_map_to_frequencies, ['min_frequency_trait'])\n",
    "        self.observe(self.setup_demo, ['min_frequency_trait'])\n",
    "\n",
    "        # text entry for max frequency\n",
    "        self.max_frequency_entry = widgets.FloatText(value=32000, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.max_frequency_entry, 'value'), (self, 'max_frequency_trait'))\n",
    "        self.observe(self.tonotopic_map_to_frequencies, ['max_frequency_trait'])\n",
    "        \n",
    "        # number of time samples to perform FFT\n",
    "        self.n_samples_window_entry = widgets.IntText(value=2048, disabled=False, layout = {'width' : '100%'})\n",
    "        traitlets.link((self.n_samples_window_entry, 'value'), (self, 'n_samples_window_trait'))\n",
    "        self.observe(self.setup_demo, ['n_samples_window_trait'])\n",
    "        \n",
    "        self.panel.processor_settings = {'Time to run (s): ' : self.time_to_run_entry,\n",
    "                                'Number of LEDs: ' : self.n_leds_entry,\n",
    "                                'Number of FFT bins: ' : self.n_fft_bins_entry,\n",
    "                                'Minimum frequency (Hz): ' : self.min_frequency_entry,\n",
    "                                'Maximum frequency (Hz): ' : self.max_frequency_entry,\n",
    "                                'Number of samples in window: ' : self.n_samples_window_entry}\n",
    "        \n",
    "        # PLOTS SETUP\n",
    "        # frequency domain plot\n",
    "        # divide by 2 as we will only look at positive frequencies\n",
    "        buffer_ft = np.zeros(int(self.n_fft_bins_trait/2))\n",
    "        ft_mag_line = px.line(x = np.linspace(0, int(self.panel.fs_trait/2), int(self.n_fft_bins_trait/2)),\n",
    "                              y = buffer_ft)\n",
    "        self.panel.fig_ft = go.FigureWidget(ft_mag_line)\n",
    "        \n",
    "        self._setup_fft_plot(0)\n",
    "        \n",
    "        # generate frequency map from image of tonotopic map\n",
    "        self.tonotopic_map_to_frequencies(0)\n",
    "        \n",
    "        # setup audio processing with FFT demo\n",
    "        self.setup_demo(0)\n",
    "        \n",
    "        # function to update main control panel log - linked automatically if dashboard is used\n",
    "        self.update_main_control_panel_log = None\n",
    "        \n",
    "        # load LED driver and set up linked array of currents\n",
    "        self.dac_ref_trait = 6.25\n",
    "        self.switch_max_current_trait = 130\n",
    "        self.device_max_current_trait = 2500\n",
    "        self.observe(self.setup_led_driver, names=['dac_ref_trait', 'switch_max_current_trait', 'device_max_current_trait'])\n",
    "        self.setup_led_driver([])\n",
    "        \n",
    "        # Configure audio input\n",
    "        self.pAudio = ol.audio_codec_ctrl_0\n",
    "        self.pAudio.configure()\n",
    "        self.pAudio.select_line_in()\n",
    "                \n",
    "    def _setup_fft_plot(self, trait_change):\n",
    "        # frequency domain plot\n",
    "        # divide by 2 as we will only look at positive frequencies\n",
    "        buffer_ft = np.zeros(int(self.n_fft_bins_trait/2))\n",
    "        self.panel.fig_ft.data[0].update({'x' : np.linspace(0, int(self.panel.fs_trait/2), int(self.n_fft_bins_trait/2))})\n",
    "        self.panel.fig_ft.data[0].update({'y' : buffer_ft})\n",
    "\n",
    "        self.panel.fig_ft.update_layout(xaxis = {'title' : 'frequency (Hz)', 'gridcolor' : '#444444'})\n",
    "        self.panel.fig_ft.update_layout(yaxis = {'title' : 'magnitude', 'gridcolor' : '#444444'})\n",
    "        self.panel.fig_ft.update_layout(title = {'text' : 'Fourier Transform Magnitude'})\n",
    "        self.panel.fig_ft.update_layout(title_font_color='#FFFFFF')\n",
    "        self.panel.fig_ft.update_xaxes(color='#FFFFFF')\n",
    "        self.panel.fig_ft.update_yaxes(color='#FFFFFF')\n",
    "        self.panel.fig_ft.update_layout(paper_bgcolor='#212121')\n",
    "        self.panel.fig_ft.update_layout(plot_bgcolor='#212121')\n",
    "        self.panel.fig_ft.data[0].line.color = \"#635faa\"\n",
    "\n",
    "    def show_original_tonotopic_map(self):\n",
    "        plt.imshow(cv2.cvtColor(self.im, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "    def show_sampled_tonotopic_map(self):\n",
    "        plt.imshow(self.im_sampled)\n",
    "        plt.show()\n",
    "        \n",
    "    def tonotopic_map_to_frequencies(self, trait_change):\n",
    "        \"\"\"Converts an image of the tonotopic map to a frequency map. This should be changed to a better solution.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.im = cv2.imread(f'images/tonotopic_map_image.png')\n",
    "\n",
    "        self.im_sampled = self.im[10::math.ceil(np.shape(self.im)[0]/10), 0::math.ceil(np.shape(self.im)[1]/10), :]\n",
    "        self.im_sampled = cv2.cvtColor(self.im_sampled, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # go from red to blue as image is BGR\n",
    "        red = Color(\"blue\")\n",
    "        n_colours = 20\n",
    "        colours = list(red.range_to(Color(\"red\"), n_colours))\n",
    "\n",
    "        frequencies = np.linspace(start=self.min_frequency_trait, stop=self.max_frequency_trait, num=n_colours)\n",
    "\n",
    "        colours_list_rgb = [colour.rgb for colour in colours]\n",
    "        colours_image_rgb = self.im_sampled/255\n",
    "\n",
    "        # get frequency bin for each location on sampled tonotopic map\n",
    "        self.frequency_map = np.zeros((10,10))\n",
    "\n",
    "        for r in range(10):\n",
    "            for c in range(10):\n",
    "                # use euclidean distance to get frequency bins for tonotopic map image\n",
    "                idx = np.argmin(np.array([np.linalg.norm(colours_list_rgb[colour] - colours_image_rgb[r, c, :]) for colour in range(n_colours)]))\n",
    "\n",
    "                self.frequency_map[r, c] = frequencies[idx]\n",
    "                \n",
    "    def setup_led_driver(self, trait_change):\n",
    "        # updating this array will set the LEDs to the corresponding current counts value\n",
    "        self.led_counts = allocate(shape=(10,10), dtype=np.uint16)\n",
    "        self.led_counts.fill(0)\n",
    "        \n",
    "        dac_refs = {0 : 3.125,\n",
    "            1 : 6.25,\n",
    "            2 : 12.5,\n",
    "            3 : 25,\n",
    "            4 : 50,\n",
    "            5 : 100,\n",
    "            6 : 200,\n",
    "            7 : 300}\n",
    "        dac_ref = [v for v in dac_refs.values()].index(self.dac_ref_trait)\n",
    "        switch_max_current = int(self.switch_max_current_trait)\n",
    "        device_max_current = int(self.device_max_current_trait)\n",
    "        \n",
    "        led_driver_program = audio_led_driver_setup(self.ol)\n",
    "        led_driver_program.leds_configure(dac_ref, switch_max_current, device_max_current) # 6.25 mA limit\n",
    "        led_driver_program.leds_start(self.led_counts)\n",
    "        \n",
    "        if self.update_main_control_panel_log != None:\n",
    "            self.update_main_control_panel_log('Microblaze LED driver programmed\\n')\n",
    "    \n",
    "    def setup_demo(self, trait_change):\n",
    "        \"\"\"Use frequency map generated from image of tonotopic map to create mapping of FFT bins to grid.\n",
    "        \n",
    "        \"\"\"\n",
    "        # GENERATE MAPPING OF FREQUENCY TO GRID\n",
    "        self.ft_bins = np.linspace(0, self.panel.fs_trait/2, int(self.n_fft_bins_trait/2))\n",
    "\n",
    "        # location of minimum frequency in ft\n",
    "        self.idx_min = np.argmin(np.abs(self.ft_bins - self.min_frequency_trait))\n",
    "        self.idx_max = np.argmax(np.abs(self.ft_bins - self.min_frequency_trait))\n",
    "        ft_bins_in_range = np.linspace(self.ft_bins[self.idx_min], self.ft_bins[self.idx_max], 100)\n",
    "\n",
    "        self.ft_bin_indices_to_grid = [[np.argmin(np.abs(ft_bins_in_range-self.frequency_map[r,c])) for c in range(10)] for r in range(10)]\n",
    "        \n",
    "        self.sample_time = self.n_samples_window_trait / self.panel.fs_trait\n",
    "        \n",
    "        self.threshold = 10000\n",
    "        \n",
    "    def _start_button_clicked(self, button):\n",
    "        self.run_task = asyncio.ensure_future(self.run())\n",
    "        \n",
    "    async def run(self):\n",
    "        \"\"\"Run demo for specified time.\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(int(self.time_to_run_trait / self.sample_time)):\n",
    "            # time domain plot\n",
    "            self.pAudio.record(seconds=self.sample_time)\n",
    "            buffer_signed = ((self.pAudio.buffer << 8).view(np.int32) >> 8)[0::2] # change to signed int and look at one channel (mono)\n",
    "            \n",
    "            # UPDATE TIME PLOT\n",
    "            if self.panel.time_plot_pause_toggle.value == False:\n",
    "                self.panel.fig.data[0].update({'y' : np.concatenate([self.panel.fig.data[0]['y'][self.n_samples_window_trait:], buffer_signed])})\n",
    "\n",
    "            # compute fft\n",
    "            ft = abs(np.fft.fft(buffer_signed))[0:int(self.n_fft_bins_trait/2)]\n",
    "            \n",
    "            # UPDATE FREQUENCY PLOT\n",
    "            if self.panel.freq_plot_pause_toggle.value == False:\n",
    "                self.panel.fig_ft.data[0].update({'y' : ft})\n",
    "\n",
    "            ft_bin_max_values = np.array([max(arr) for arr in np.array_split(ft, 100)])\n",
    "            ft_bin_max_values_norm = ft_bin_max_values/1000000\n",
    "\n",
    "            # translate values to grid\n",
    "            ft_bin_values_on_grid = (ft_bin_max_values_norm[np.array(self.ft_bin_indices_to_grid)]*65535).astype(np.uint16)\n",
    "            ft_bin_values_on_grid[ft_bin_values_on_grid < self.threshold] = 0\n",
    "\n",
    "            # LED CONTROL ##############################\n",
    "            self.led_counts[:,:] = ft_bin_values_on_grid[:,:]\n",
    "            \n",
    "            if self.panel.stop_button.value:\n",
    "                self.panel.stop_button.value = False\n",
    "                break\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                await asyncio.sleep(0.001)\n",
    "            \n",
    "        # switch off all LEDs\n",
    "        self.led_counts.fill(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c842727e-233c-4ac6-a7d7-12c5c3d5efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hearlight import Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18bec5e4-0faa-40e2-a204-5f9348d09726",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "dashboard = Dashboard(led_control_panel = False, array_currents_panel = False, audio_control_panel = AudioProcessSoftwareFFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1d07d9-88e4-42f6-9a50-1cd05cea2594",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064264b1122442ccac0f5d5ca99a0acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x0e\\xe2\\x00\\x00\\x01<\\x08\\x06\\x00\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dashboard.banner_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9974ea4e-355d-4185-9ba9-f17736201af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create panel for images of tonotopic map\n",
    "tonotopic_map_images_panel = widgets.VBox()\n",
    "tonotopic_map_images_panel.add_class('t_map_panel_css')\n",
    "\n",
    "tonotopic_map_image_original_heading = widgets.widgets.HTML(value='<b>&nbsp;Original tonotopic map</b>')\n",
    "tonotopic_map_image_original_heading.add_class('section_heading')\n",
    "tonotopic_map_images_panel.children += (tonotopic_map_image_original_heading,)\n",
    "\n",
    "tonotopic_map_image_original_box = widgets.Box()\n",
    "tonotopic_map_image_original_image = widgets.Image(value=open('images/tonotopic_map_image_orig.png', 'rb').read())\n",
    "tonotopic_map_image_original_image.add_class('t_map_image_css')\n",
    "tonotopic_map_image_original_box.children += (tonotopic_map_image_original_image,)\n",
    "tonotopic_map_image_original_box.add_class('t_map_image_box_css')\n",
    "tonotopic_map_images_panel.children += (tonotopic_map_image_original_box,)\n",
    "\n",
    "tonotopic_map_image_sampled_heading = widgets.widgets.HTML(value='<b>&nbsp;Sampled tonotopic map</b>')\n",
    "tonotopic_map_image_sampled_heading.add_class('section_heading')\n",
    "tonotopic_map_images_panel.children += (tonotopic_map_image_sampled_heading,)\n",
    "\n",
    "tonotopic_map_image_sampled_box = widgets.Box()\n",
    "tonotopic_map_image_sampled_image = widgets.Image(value=open('images/tonotopic_map_image_sampled.png', 'rb').read())\n",
    "tonotopic_map_image_sampled_image.add_class('t_map_image_css')\n",
    "tonotopic_map_image_sampled_box.children += (tonotopic_map_image_sampled_image,)\n",
    "tonotopic_map_image_sampled_box.add_class('t_map_image_box_css')\n",
    "tonotopic_map_images_panel.children += (tonotopic_map_image_sampled_box,)\n",
    "\n",
    "#tonotopic_map_images_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33119d5a-f3b7-4467-9700-4d25a5e1a7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7696745b651f4b72b289e3926a2946f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value='<h1 style=\"font-size:1.7em\"><b>HearLight PYNQ control syst…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import GridspecLayout\n",
    "\n",
    "dashboard_box = GridspecLayout(12, 12, height='850px', width='1465px', grid_gap=\"10px\")\n",
    "\n",
    "dashboard_box[0:5,0:2] = dashboard.info_panel\n",
    "\n",
    "dashboard_box[0:5,2:12] = dashboard.main_control_panel\n",
    "\n",
    "dashboard_box[5:12,0:10] = dashboard.audio_control_panel\n",
    "\n",
    "dashboard_box[5:12,10:12] = tonotopic_map_images_panel\n",
    "\n",
    "dashboard_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a36e7-f38f-4d03-9391-2030b1cbabae",
   "metadata": {},
   "source": [
    "# DEMO: LED control through audio input using FFT\n",
    "--- \n",
    "*May take a few minutes to load...*\n",
    "\n",
    "#### Instructions\n",
    "- Connect audio input to *LINE-IN* on PYNQ-Z2 board\n",
    "- Start the program by pressing ***START*** on the audio control panel\n",
    "- The audio signal is windowed and an FFT processes samples in real-time to control LEDs based on the tonotopic map of the mouse auditory cortex\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
